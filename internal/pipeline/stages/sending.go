package stages

import (
	"context"
	"fmt"
	"log"
	"net"
	"time"

	"github.com/nashabanov/ueba-event-generator/internal/domain/event"
	"github.com/nashabanov/ueba-event-generator/internal/metrics"
	"github.com/nashabanov/ueba-event-generator/internal/network"
	"github.com/nashabanov/ueba-event-generator/internal/workers"
)

// NetworkSendingStage —Ä–µ–∞–ª–∏–∑—É–µ—Ç SendingStage –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø–æ —Å–µ—Ç–∏
type NetworkSendingStage struct {
	name         string
	destinations []string // ["127.0.0.1:514", "10.0.0.1:514"]
	protocol     string   // "udp" –∏–ª–∏ "tcp"
	timeout      time.Duration

	tcpPool    *network.TCPConnectionPool
	workerPool *workers.WorkerPool
	udpConn    net.Conn
	metrics    *metrics.PerformanceMetrics
	input      chan event.Event
}

func NewNetworkSendingStage(name string) *NetworkSendingStage {
	workerPool := workers.NewWorkerPool(0, 5000, func() workers.JobBatch {
		return &NetworkSendJobBatch{
			data: make([]*SerializedData, 0, 50), // –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —ë–º–∫–æ—Å—Ç—å
		}
	})
	workerPool.SetPoolType("network")
	return &NetworkSendingStage{
		name:         name,
		destinations: []string{"127.0.0.1:514"},
		protocol:     "udp",
		timeout:      5 * time.Second,
		workerPool:   workerPool,
		metrics:      metrics.NewPerformanceMetrics(),
		input:        make(chan event.Event, 1000),
	}
}

func (s *NetworkSendingStage) Name() string {
	return s.name
}

type NetworkSendJobBatch struct {
	stage *NetworkSendingStage
	data  []*SerializedData
}

func (jb *NetworkSendJobBatch) ExecuteBatch() error {
	if jb == nil {
		log.Printf("‚ùå CRITICAL: NetworkSendJobBatch is nil!")
		return fmt.Errorf("job batch is nil")
	}

	for _, d := range jb.data {
		if err := jb.stage.SendData(d); err != nil {
			// –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ ‚Äî –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —á–µ—Ä–µ–∑ debug-—Ñ–ª–∞–≥)
			// log.Printf("Failed to send event %s: %v", d.ID, err)

			// –û—à–∏–±–∫–∏ —É–∂–µ —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ SendData(),
			// –ø–æ—ç—Ç–æ–º—É –∑–¥–µ—Å—å –Ω–∏—á–µ–≥–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –¥–µ–ª–∞—Ç—å –Ω–µ –Ω—É–∂–Ω–æ
		}
	}

	// –í–∞–∂–Ω–æ: –æ—á–∏—â–∞–µ–º —Å–ª–∞–π—Å, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —ë–º–∫–æ—Å—Ç—å –¥–ª—è –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
	jb.data = jb.data[:0]

	return nil
}

func (s *NetworkSendingStage) Run(ctx context.Context, in <-chan *SerializedData, out chan<- *SerializedData, ready chan<- bool) error {
	// –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
	var err error
	if s.protocol == "tcp" && len(s.destinations) > 0 {
		poolSize := 12
		s.tcpPool, err = network.NewTCPConnectionPool(s.destinations[0], poolSize)
		if err != nil {
			return fmt.Errorf("failed to create TCP connection pool: %w", err)
		}
		log.Printf("‚úÖ TCP connection pool created: %d connections to %s", poolSize, s.destinations[0])
	} else if s.protocol == "udp" && len(s.destinations) > 0 {
		s.udpConn, err = net.Dial("udp", s.destinations[0])
		if err != nil {
			return fmt.Errorf("failed to create UDP socket to %s: %w", s.destinations[0], err)
		}
		s.udpConn.SetWriteDeadline(time.Now().Add(s.timeout))
		log.Printf("‚úÖ UDP socket created to %s", s.destinations[0])
	}

	// –ó–∞–ø—É—Å–∫–∞–µ–º worker pool
	s.workerPool.Start(ctx)

	// üî• –°–∏–≥–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –î–û –Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
	if ready != nil {
		close(ready)
	}

	defer s.workerPool.Stop()
	defer s.closeConnections()

	const batchSize = 50
	const batchTimeout = 5 * time.Millisecond

	var (
		currentBatch *NetworkSendJobBatch
		timer        *time.Timer
		timerC       <-chan time.Time
	)

	for {
		select {
		case serializedData, ok := <-in:
			if !ok {
				// –ö–∞–Ω–∞–ª –∑–∞–∫—Ä—ã—Ç ‚Äî –∑–∞–≤–µ—Ä—à–∞–µ–º
				if currentBatch != nil && len(currentBatch.data) > 0 {
					s.workerPool.Submit(currentBatch)
				}
				// –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞...
				return nil
			}

			if currentBatch == nil {
				currentBatch = s.workerPool.GetJob().(*NetworkSendJobBatch)
				currentBatch.stage = s
				currentBatch.data = currentBatch.data[:0]
				timer = time.NewTimer(batchTimeout)
				timerC = timer.C
			}

			currentBatch.data = append(currentBatch.data, serializedData)

			if len(currentBatch.data) >= batchSize {
				if !s.workerPool.Submit(currentBatch) {
					metrics.GetGlobalMetrics().IncrementDropped()
				}
				currentBatch = nil
				if timer != nil {
					timer.Stop()
					timer = nil
					timerC = nil
				}
			}

		case <-timerC:
			if currentBatch != nil && len(currentBatch.data) > 0 {
				if !s.workerPool.Submit(currentBatch) {
					metrics.GetGlobalMetrics().IncrementDropped()
				}
				currentBatch = nil
			}
			timer = nil
			timerC = nil

		case <-ctx.Done():
			if currentBatch != nil && len(currentBatch.data) > 0 {
				s.workerPool.Submit(currentBatch)
			}
			// –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞...
			return ctx.Err()
		}
	}
}

// SendData –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ —Å–µ—Ç–∏
func (s *NetworkSendingStage) SendData(data *SerializedData) error {
	globalMetrics := metrics.GetGlobalMetrics()

	destination := data.Destination
	if destination == "" && len(s.destinations) > 0 {
		destination = s.destinations[0]
	}

	protocol := data.Protocol
	if protocol == "" {
		protocol = s.protocol
	}

	var err error
	switch protocol {
	case "udp":
		err = s.sendUDP(destination, data.Data)
	case "tcp":
		err = s.sendTCP(destination, data.Data)
	default:
		err = fmt.Errorf("unsupported protocol: %s", protocol)
	}

	if err != nil {
		globalMetrics.IncrementFailed()
		return err
	}

	globalMetrics.IncrementSent()
	return nil
}

// sendUDP –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ UDP (stateless) - –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
func (s *NetworkSendingStage) sendUDP(destination string, data []byte) error {
	if s.udpConn == nil {
		return fmt.Errorf("UDP socket not initialized")
	}

	globalMetrics := metrics.GetGlobalMetrics()
	globalMetrics.IncrementConnections() // –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫
	defer globalMetrics.DecrementConnections()

	// –û–±–Ω–æ–≤–ª—è–µ–º —Ç–∞–π–º–∞—É—Ç –Ω–∞ –∫–∞–∂–¥—ã–π –≤—ã–∑–æ–≤ (–Ω–∞ —Å–ª—É—á–∞–π –¥–æ–ª–≥–æ–π —Ä–∞–±–æ—Ç—ã)
	s.udpConn.SetWriteDeadline(time.Now().Add(s.timeout))

	_, err := s.udpConn.Write(data)
	if err != nil {
		globalMetrics.IncrementTimeouts()
		return fmt.Errorf("failed to write UDP data to %s: %w", destination, err)
	}

	return nil
}

func (s *NetworkSendingStage) sendTCP(destination string, data []byte) error {
	if s.tcpPool == nil {
		return fmt.Errorf("TCP connection pool not initialized for destination %s", destination)
	}

	globalMetrics := metrics.GetGlobalMetrics()

	conn := s.tcpPool.GetConnection()
	if conn == nil {
		globalMetrics.IncrementTimeouts()
		return fmt.Errorf("no healthy TCP connections available to %s", destination)
	}

	dataWithNewline := append(data, '\n')

	written := 0
	for written < len(dataWithNewline) {
		n, err := conn.Write(dataWithNewline[written:])
		if err != nil {
			conn.MarkUnhealthy()
			globalMetrics.IncrementTimeouts()
			return fmt.Errorf("failed to write TCP data to %s: %w", destination, err)
		}
		written += n
	}

	return nil
}

func (s *NetworkSendingStage) closeConnections() {
	if s.tcpPool != nil {
		s.tcpPool.Close()
		log.Printf("TCP connection pool closed")
		s.tcpPool = nil
	}

	if s.udpConn != nil {
		s.udpConn.Close()
		log.Printf("UDP socket closed")
		s.udpConn = nil
	}
}

func (s *NetworkSendingStage) GetConnectionStats() (total, healthy int) {
	if s.tcpPool != nil {
		return s.tcpPool.GetStats()
	}
	return 0, 0
}

func (s *NetworkSendingStage) GetConnectionPoolInfo() string {
	if s.tcpPool == nil {
		return "TCP pool: not initialized"
	}

	total, healthy := s.tcpPool.GetStats()
	return fmt.Sprintf("TCP pool: %d/%d healthy connections", healthy, total)
}

// –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã
func (s *NetworkSendingStage) SetDestinations(destinations []string) error {
	if len(destinations) == 0 {
		return fmt.Errorf("destinations cannot be empty")
	}

	for _, dest := range destinations {
		if _, _, err := net.SplitHostPort(dest); err != nil {
			return fmt.Errorf("invalid destination address %s: %w", dest, err)
		}
	}

	if s.tcpPool != nil && len(destinations) > 0 && destinations[0] != s.destinations[0] {
		log.Printf("üîÑ Destination changed from %s to %s, TCP pool will be recreated on next run",
			s.destinations[0], destinations[0])
		s.closeConnections() // –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å—Ç–∞—Ä—ã–π pool
	}

	s.destinations = destinations
	return nil
}

func (s *NetworkSendingStage) SetProtocol(protocol string) error {
	if protocol != "udp" && protocol != "tcp" {
		return fmt.Errorf("unsupported protocol: %s (supported: udp, tcp)", protocol)
	}

	// ‚úÖ –ï–°–õ–ò –º–µ–Ω—è–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª - –∑–∞–∫—Ä—ã–≤–∞–µ–º TCP pool
	if s.protocol == "tcp" && protocol != "tcp" {
		s.closeConnections()
		log.Printf("üîÑ Protocol changed from TCP to %s, connection pool closed", protocol)
	}

	s.protocol = protocol
	return nil
}

func (s *NetworkSendingStage) ResizeConnectionPool(newSize int) error {
	if s.tcpPool == nil {
		return fmt.Errorf("TCP connection pool not initialized")
	}

	if newSize < 1 || newSize > 50 {
		return fmt.Errorf("invalid pool size %d, must be between 1 and 50", newSize)
	}

	total, healthy := s.tcpPool.GetStats()
	log.Printf("üîß TCP pool resize requested: current=%d healthy/%d total, requested=%d",
		healthy, total, newSize)
	log.Printf("‚ö†Ô∏è Pool resizing not implemented - requires restart to change size")

	return fmt.Errorf("pool resizing not implemented - restart application with new configuration")
}

func (s *NetworkSendingStage) RecreateUnhealthyConnections() int {
	if s.tcpPool == nil {
		return 0
	}

	total, healthy := s.tcpPool.GetStats()
	unhealthy := total - healthy

	if unhealthy > 0 {
		log.Printf("üîÑ Attempting to recreate %d unhealthy connections", unhealthy)
		return unhealthy
	}

	return 0
}

// –ú–µ—Ç–æ–¥—ã –º–µ—Ç—Ä–∏–∫
func (s *NetworkSendingStage) GetSentCount() uint64 {
	_, sent, _, _ := metrics.GetGlobalMetrics().GetStats()
	return sent
}

func (s *NetworkSendingStage) GetFailedCount() uint64 {
	_, _, failed, _ := metrics.GetGlobalMetrics().GetStats()
	return failed
}

func (s *NetworkSendingStage) GetStageStats() map[string]interface{} {
	_, sent, failed, dropped := metrics.GetGlobalMetrics().GetStats()

	stats := map[string]interface{}{
		"stage_name":          s.name,
		"protocol":            s.protocol,
		"destinations":        len(s.destinations),
		"events_sent":         sent,
		"events_failed":       failed,
		"events_dropped":      dropped,
		"worker_pool_healthy": s.workerPool != nil,
	}

	if s.tcpPool != nil {
		total, healthy := s.tcpPool.GetStats()
		stats["tcp_connections_total"] = total
		stats["tcp_connections_healthy"] = healthy
		stats["tcp_pool_efficiency"] = float64(healthy) / float64(total) * 100.0
	}

	return stats
}

func (s *NetworkSendingStage) IsHealthy() (bool, string) {
	if s.workerPool == nil {
		return false, "worker pool not initialized"
	}

	if s.protocol == "tcp" {
		if s.tcpPool == nil {
			return false, "TCP protocol selected but connection pool not initialized"
		}

		total, healthy := s.tcpPool.GetStats()
		if healthy == 0 {
			return false, fmt.Sprintf("no healthy TCP connections (0/%d)", total)
		}

		efficiency := float64(healthy) / float64(total)
		if efficiency < 0.5 {
			return false, fmt.Sprintf("TCP connection pool degraded: %d/%d healthy (%.1f%%)",
				healthy, total, efficiency*100)
		}
	}

	return true, "stage healthy"
}

func (s *NetworkSendingStage) GetOptimizationRecommendations() []string {
	var recommendations []string

	_, sent, failed, dropped := metrics.GetGlobalMetrics().GetStats()

	if dropped > 0 {
		dropRate := float64(dropped) / float64(float64(sent)+float64(failed)+float64(dropped)) * 100
		if dropRate > 1.0 {
			recommendations = append(recommendations,
				fmt.Sprintf("High drop rate (%.1f%%) - consider increasing worker pool queue size", dropRate))
		}
	}

	if failed > 0 {
		failRate := float64(failed) / float64(sent+failed) * 100
		if failRate > 5.0 {
			recommendations = append(recommendations,
				fmt.Sprintf("High failure rate (%.1f%%) - check network connectivity and destination availability", failRate))
		}
	}

	if s.protocol == "tcp" && s.tcpPool != nil {
		total, healthy := s.tcpPool.GetStats()
		if healthy < total {
			recommendations = append(recommendations,
				fmt.Sprintf("TCP connections degraded (%d/%d) - consider connection timeout tuning", healthy, total))
		}

		if total < 5 && sent > 50000 {
			recommendations = append(recommendations,
				"High load with few TCP connections - consider increasing connection pool size")
		}
	}

	if len(recommendations) == 0 {
		recommendations = append(recommendations, "Stage operating optimally - no recommendations")
	}

	return recommendations
}
